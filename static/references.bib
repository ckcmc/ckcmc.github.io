@article{beiswenger_cmc-core_2020,
        title = {{CMC}-core: a schema for the representation of {CMC} corpora in {TEI}},
        copyright = {© Tous droits réservés},
        issn = {1638-9808},
        shorttitle = {{CMC}-core},
        url = {http://journals.openedition.org/corpus/4553},
        doi = {10.4000/corpus.4553},
        abstract = {In this Paper, we describe a schema and models which have been developed for the representation of corpora of computer-mediated communicatin (CMC corpora) using the representation framework provided by the Text Encoding Initiative (TEI). We characterise CMC discourse as dialogic, sequentially organised interchange between humans and point out that many features of CMC are not adequately handled by current corpus encoding schemas and tools. We formulate desiderata for a representation of CMC in encoding schemes and argue why the TEI is a suitable framework for the encoding of CMC corpora. We propose a model of basic CMC units (utterances, posts, and nonverbal activities) and the macro- and micro-level structures of interactions in CMC environments. Based on these models, we introduce CMC-core, a TEI customisation for the encoding of CMC corpora, which defines CMC-specific encoding features on the four levels of elements, model classes, attribute classes, and modules of the TEI infrastructure. The description of our customisation is illustrated by encoding examples from corpora by researchers of the TEI SIG CMC, representing a variety of CMC genres, i.e. chat, wiki talk, twitter, blog, and Second Life interactions. The material described, i.e. schemata, encoding examples, and documentation, is available from the of the TEI CMC SIG Wiki and will accompany a feature request to the TEI council in late 2019.},
        language = {fr},
        number = {20},
        urldate = {2020-11-18},
        journal = {Corpus},
        author = {Beißwenger, Michael and Lüngen, Harald},
        month = jan,
        year = {2020},
}

@article{wilkinson_fair_2016,
	title = {The {FAIR} {Guiding} {Principles} for scientific data management and stewardship},
	volume = {3},
	url = {http://www.nature.com/articles/sdata201618},
	doi = {10.1038/sdata.2016.18},
	abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
	journal = {Scientific Data},
	author = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, IJsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and da Silva Santos, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Mercè and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J.G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and ’t Hoen, Peter A.C and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and van Schaik, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and van der Lei, Johan and van Mulligen, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
	month = mar,
	year = {2016},
	pages = {160018--160018},
}

@book{WAC-X:2016,
  type = {proceedings},
  title = {Proceedings of the 10th {{Web}} as {{Corpus Workshop}} ({{WAC}}-{{X}}) and the {{EmpiriST Shared Task}}},
  editor = {Cook, Paul and Evert, Stefan and Sch{\"a}fer, Roland and Stemle, Egon},
  year = {2016},
  month = aug,
  publisher = {{Association for Computational Linguistics}},
  address = {{Berlin}},
  url = {http://aclweb.org/anthology/W16-26},
	abstract = {The World Wide Web has become increasingly popular as a source of
		linguistic data, not only within the NLP communities, but also with
		theoretical linguists facing problems of data sparseness or data diversity.
		Accordingly, web corpora continue to gain importance, given their size and
		diversity in terms of genres/text types. The field is still new, though, and a
		number of issues in web corpus construction ne ed much additional research,
		both fundamental and applied. These issues range from questions of corpus
		design (e.g., assessment of corpus composition, sampling strategies and their
		relation to crawling algorithms , and handling of duplicated material) to more
		technical aspects (e.g., efficient implementation of individual post-processing
		steps in document cleaning and linguistic annotation, or large-scale
		parallelization to achieve web-scale corpus construction). Similarly, the
		systematic evaluation of web corpora, for example in the form of task based
		comparisons to traditional corpora, has only recently shifted into focus. For
		almost a decade, the ACL SIGWAC (http://www.sigwac.org.uk/), and especially the
		highly successful Web as Corpus (WAC) workshops have served as a platform for
		researchers interested in compilation, processing and application of
		web-derived corpora. Past workshops were co-located with major conferences on
		computational linguistics and/or corpus linguistics (such as EACL, NAACL, LREC,
		WWW, and Corpus Linguistics). WAC-X als o featured the final workshop of the
		EmpiriST 2015 shared task "Automatic Linguistic Annotation of Computer-Mediated
		Communication / Social Media" (see https://sites.google.com/site/empirist2015/
		for details) and the panel discussion "Corpora, open science, and copyright
		reforms" (see https://www.sigwac.org.uk/wiki/WAC-X\#paneldisc for details).},
  copyright = {All rights reserved},
  isbn = {978-1-945626-15-9},
}
